---
title: Introduction to Convolutional Neural Networks for Visual Recognition
date: 2020-07-03
description: Lecture 1 of CS231n by Stanford University
categories:
    - cs231n
    - computer vision
image: https://user-images.githubusercontent.com/33539468/86482554-28287580-bd8d-11ea-81d5-46daa259fa63.jpg

---



CS231n is all about Computer Vision, which is the study of visual data. The amount of visual data in this world has really exploded to a ridiculous degree in the last couple of years. And this is largely a result of the large number of sensors in the world. As result of these sensors, such as cameras in smartphones, there's massive amount of visual data being produced out there in the world each day. In 2015 study from Cisco, roughly 80% of all traffic on the internet would be video by 2017. This is not even counting all the images and other types of visual data on  the web. So it's really critical to develop algorithms that can utilize and understand this data.

However there's a problem with visual data, and that it's really hard to understand. Sometimes we call visual data the dark matter of the internet in analogy with dark matter in physics. Dark matter accounts for some astonishingly large fraction of the mass in the universe, and we know about it due to the existence of gravitational pulls on various celestial bodies, but we can't directly observe it. And, visual data on the internet is much the same where it comprises the majority of bits flying around the internet, but it's very difficult for algorithms to actually go in and understand what exactly is comprising all the visual data on the web. It's really crucial that we develop technologies that can dive in and automatically understand the content of visual data.

So this field of computer vision is truly interdisciplinary field, and it touches on many different areas of science, engineering and technology. Obviously, computer vision is the center of the universe, but sort of as a constellation of fields around computer vision, we touch on areas like physics because we need to understand optics and image formation and how images are actually physically formed. We need to understand biology and psychology to understand how animal brains physically see and process visual information. We of course draw a lot on computer science, mathematics and engineering as we actually strive to build computer systems that implement our computer vision algorithms.

The history of vision go back many years ago in fact about 543 million years ago. The earth was mostly water and there were a few species of animals floating around in the ocean and life was very chill. Animals wouldn't move around much. Since they don't have eyes or visually sensible organs, the only way to get food is just grabbing it when it swims by them. But something really remarkable happened around 540 million years ago. From fossil studies, zoologists found out within a very short period of time - 10 million years - the number of animal species just exploded. What caused this? It was mystery, and evolutionary biologists call this as 'evolution's Big Bang'. Few years ago, an Austrailian zoologist called Andrew Parker proposed one of the most convincing theory from the studies of fossils. He discovered the first animals developed eyes and the onset of vision started this explosive speciation phase. Once you can see life becomes much more proactive. Some predators went after prey and prey have to escape from predators. After 540 million years vision has developed into the biggest sensory system of almost all anmials especially intelligent animals.

What the history of humans making mechanical vision or cameras? One of the early cameras that we know today is from the 1600s, the Renaissance period of time. Pinhole camera theory was developed at that time. It's very similar to the early eyes that animals developed with a hole that collects lights and then a plane in the back of the camera that collects the information and project the imagery. As cameras evolved, today they are the most popular sensors people use from smartphones to other sensors. In the meantime, biologist started studying the mechanism of vision. One of the most influential work in both human vision or animal vision as well as that inspired computer vision is the work done by Hubel and Wiesel in the 50s and 60s using electrophysiolgy. What they were asking, the question is "what was the visual processing like in mammals", so they chose to study cat brain which is more or less similar to human brain from a visual processing point of view. What they did is to stick some electrodes in the back of the cat brain which is where the primary visual cortex area is and then look at what stimuli makes the neurons in there respond excitedly. What they learned is that there are many types of cells in the primary visual cortex, but one of the most important cell is the simple cells. They respond to oriented edges when they move in certain directions. Of course there are also more complex cells but by and large what they discovered is visual processing starts with simple structure of the visual world, oriented edges and as information moves along the visual processing pathway the brain builds up the complexity of the visual information until it can recognize the complex visual world.


![lec1_1](https://user-images.githubusercontent.com/33539468/86481661-64f36d00-bd8b-11ea-9f97-3648b2e330c5.jpg)



So the history of computer vision also starts around early 60s. Block World is a set of work published by Larry Roberts which is widely known as one of the first PhD thesis of computer vision where the visual world was simplified into simple geometric shapes. The goal is to be able to recognize them and reconstruct what these shapes are. In 1966, there was a MIT summer project called "The Summer Vision Project." The goal of it, "is an attempt to use our summer workers effectively in a construction of a significant part of a visual system." Fifty years have passed; the field of computer vision has blossomed from one summer project into a field of thousands of researchers worldwide still working on some of the most fundamental problems of vision. We still have not yet solved vision but it has grown into one of the most important and fastest growing areas of artificial intelligence. Another person that we should pay tribute to is David Marr. He was a MIT vision scientist and he has written an influential book in the late 70s about what he thinks vision is and how we should go about computer vision and develop algorithms that can enable computers to recognize the visual world. The thought process in his book is that in order to take an image and arrive at a final holistic full 3d representation of the visual world, we have to go through several process. The first process is what he calls, "primal sketch;" this is where the edges, the bars, the ends, the virtual lines, the curves, the boundaries, are represented and this is very much inspired by neuroscientists have seen: Hubel and Wiesel told us the early stage of visual processing has a lot to do with simple structures like edges. Then the next step after the edges and curves is what David Marr calls "two-and-a-half D sketch" this is where we start to piece together the surfaces, the depth information, the layers, or the discontinuities of the visual scene. And then eventually we put everything together and have a 3d model hierarchically organized in terms of surface and volumetric primitives and so on. So that was a very idealized thought process of what vision is and this way of thinking actually has dominated computer vision for several decades and is also a very intuitive way for students to enter the field of vision and think about how we can deconstruct the visual information. 

![lec1_2](https://user-images.githubusercontent.com/33539468/86481751-91a78480-bd8b-11ea-8313-43e13749a9c2.jpg)

Another very important seminal group of work happened in the 70s where people began to ask the question "how we can move beyond the simple block world and start recognizing or representing real world objects?" Think about the 70s, it's the time that there's very little data available; computers are extremely slow, PCs are not even around, but computer scientists are starting to think about how we can recognize and represent objects. So in Palo Alto both at Stanford as well as SRI, two groups of scientists proposed similar ideas: one is called "generalized cylinder," the other is called "pictorial structure." The basic idea is that every object is composed of simple geometric primitives; for example a person can be pieced together by generalized cylindrical shapes or a person can be pieced together by critical part in their elastic distance between these parts. So either representation is a way to reduce the complex structure of the object into a collection of simpler shapes and their geometric configuration. 

In the 80s, here is another example of thinking how to reconstruct of recognize the visual world from simple world structures. This work is done by David Lowe, which he tries to recognize razors by constructing lines, edges and mostly combination of straight lines. 

Despite of all of these ambitious attempts, not a lot of progress have been made in terms of delivering something that can work in real world. One important question came around is: if the object recognition is too hard, maybe we should first do object segmentation. That is the task of taking an image and group the pixels into meaningful areas. We might not know the pixels that group together is called a person, but we can extract out all the pixels belong to the person from its background; that is called image segmentation. There was an attempt solve this problem using a graph theory algorithm. 

Around the time of 1999 to 2000 machine learning techniques, especially statistical machine learning techniques start to gain momentum. These are techniques such as support vector machines, boosting, graphical models, including the first wave of neural networks. One particular work that made a lot of contribution was using AdaBoost algorithm to do real-time face detection by Paul Viola and Michael Jones. They were albe to do face detection in images in near-real-time and after the publication of this paper, Fujifilm rolled out the first digital camera that has a real-time face detector in the camera. 

There is a  seminal work by David Lowe called SIFT feature. To match the entire object to another stop sign is very difficult because there might be all kinds of changes due to camera angles, occlusion, viewpoint, lighting, and just the intrinsic variation of the object itself. But he's inspired to observe that there are some parts of the object, some features, that tend to remain diagnostic and invariant to changes. So the task of object recognition began with identifying these critical features on the object and then match the features to a similar object, that is an easier task than pattern matching the entire object. Here is a figure from his paper where it shows that a handful, several dozen SIFT features from one stop sign are identified and matched to the SIFT features of another stop sign. 

![lec1_3](https://user-images.githubusercontent.com/33539468/86481756-92d8b180-bd8b-11ea-94e4-18b9cc8df9e6.jpg)

Spatial Pyramid Matching focuses on that there are features in the images that can give us clues about which type of scene it is, whether it is a landscape or a kitchen or a highway and so on. This particular work takes these features from different parts of the image and in different resolutions and put them together in a feature descriptor and then we do support vector machine algorithm on top of that. 

Most of the ml algorithms are very likely to be overfitted in the training process and part of the problem is that visual data is very complex. Because of it's complexity, models tend to have a high dimension of input and have a lot of parameters to fit. When we don't have enough training data overfitting happens very fast and then we cannot generalize very well. So motivated by this dual reason, one is just want to recognize the world of all the objects, the other one is to overcome the machine learning bottleneck of overfitting, the project called ImageNet was started. It is one of the biggest dataset produced in the field of AI and it began to push forward the algorithm development of object recognition into another phase. According to ImageNet Large Scale Visual Recognition Challenge results from 2010  to 2012, the error rate was dropped almost 10% in 2012. That drop was very significant and the winning algorithm of that year is a convolutional neural network that beat all other algorithms around that time. And this is the focus of CS231n, including visual recognition, image classification, object detection, and image captioning. The setup in object detection is a little bit different. Rather than classifying an entire image as a cat or a dog or whatnot, instead we want to draw bounding boxes and point out the specific locations of objects in the image. Image captioning is a process by which the system produces natural language sentences when an image is given to the system. 

One of the sort of foundational works in this area of convolutional neural networks was actually in the '90s from Jan LeCun and collaborators who at that time were at Bell Labs. In 1998 they built this CNN for recognizing digits. They wanted to deploy this and wanted to be able to automatically recognize handwritten checks or addresses for the post office. They built neural networks which could take in the pixels of an image and then classify either what digit it was or what letter it was or whatnot. The networks were comprised of many layers of convolution and sub-sampling.