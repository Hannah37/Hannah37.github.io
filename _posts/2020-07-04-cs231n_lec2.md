---
title: Image classification
date: 2020-07-04
description: Lecture 2 of CS231n by Stanford University
categories:
    - cs231n
    - computer vision
image: https://user-images.githubusercontent.com/33539468/86482554-28287580-bd8d-11ea-81d5-46daa259fa63.jpg

---

Image Classification is really a core task in computer vision. A little bit more concretely, a system receives some input image, such as a cute cat as an example, and the system is aware of some predetermined set of categories or labels. These might be a dog or a cat or a truck, and there's some fixed set of category labels. The job of the computer is to look at the picture and assign it one of these fixed category labels. This is a easy task for human because so much of our own visual system in a brain is hardwired to doing these, sort of visual recognition tasks. However, it is a hard problem for a machine. It definitely doesn't get this holistic idea of a cat that you see when you look at it.



## Challenges of Image Classification: 
### a viewpoint, illumination, deformation, occlusion, background clutter, intraclass variation

![IMG-0004](https://user-images.githubusercontent.com/33539468/86518095-701cca80-be69-11ea-82b9-9e792f11eea4.jpg)

And the computer really is representing the image as this gigantic grid of numbers. Each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. Since this is just a gigantic group of numbers to a computer, it's very difficult to distill the cat-ness out of this giant array. So, we refer to this problem as the semantic gap. This idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image. You can even change the picture in a subtle ways that will cause this pixel grid to change entirely. For example, we moved the camera to the other side, then every single grid, every single pixel in this giant grid of numbers would be completely different. But, somehow, it's still representing the same cat. Thus, an algorithm for image classification need to be robust to this. 

But not only viewpoint is one problem, another is illumination. There can be different light conditions going on in a scene. Whether the cat is appearing in a dark, moody scene, or like in a very bright, sunlit scene, it's still a cat. Objects can also deform. Cats are, maybe, among the more deformable of animals that you might see out there. They can really assume a lot of different, varied poses and positions. There can also be problems of occlusion, where you might only see part of a cat, such as only the face, or just a tail peeking out from under the couch cushion. There can also be problems of background clutter, where maybe the foreground object of the cat could actually look quite similar in appearance to the background. Another problem is a Intraclass variation. One notion of cat-ness actually spans a lot of visual appearance. They can come in different shapes, sizes, colors and ages. And our algorithms need to be robust to these challenges and should be able to handle all of these problems. 


## Prediction steps based on data-driven approach

Computer programs should deal with all of these problems, all simultaneously, not just for cats but for any object category people can imagine. This is a fantastically challenging problem. Once we get a large amount of labeled dataset, we can train machine learning classifier that is going to ingest all of the data, summarize it in some way and then spit out a model that summarizes the knowledge of how to recognize these different object categories. Finally, we use this model to apply it on new images that will be able to recognize cats and dogs. 

Rather than a single function that just inputs an image can recognizes a cat, we have 2 functions. One is called train, that's going to input images and labels and then output a model. Another function called, predict, which will input the model and then make predictions for images. This is the key insight that allowed all these things to start working really well over the last 10 - 20 years or so. This idea of data-driven approach is much more general than deep learning. 

## Nearest Neighbor

Nearest Neighbor algorithm is probably the simplest classifier. During the training step it memorizes all of the training data. And during the prediction step, it is going to take some new image and try to find the most similar image in the training data to that new image. Finally it predicts the label of that most similar image. 

![IMG-0005](https://user-images.githubusercontent.com/33539468/86518096-714df780-be69-11ea-80f2-7a95dc3dbdf1.jpg)

L1 distance, also called the the Manhattan distance, is a sort of simple idea for comparing images. We're going to just compare individual pixels in these images. Supposing that our test image is maybe just a tiny 4*4 image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. And then, sum all these up across all the pixels in the image. 

If we have N examples in training set, how fast can we expect training and testing to be? Training is constant, O(1), because a classifier don't really need to do anything. It just need to memorize data. But now, at test time we need to do this comparison stop and compare a test image to each of the N training examples in the dataset. So it's going to be O(N). This is actually somewhat backwards. In practice, people want classifiers that are fast at prediction on their mobile phones or browsers. From this perspective, nearest neighbor algorithm is a little bit backwards. 

## K-Nearest Neighbors

![IMG-0006](https://user-images.githubusercontent.com/33539468/86518097-71e68e00-be69-11ea-90ae-f660fd8810b7.jpg)

Rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find k of our nearest neighbors according to our distance metric. And then take a vote among each of our neighbors. The simplest way to take a vote is a majority vote. Once we move to K=3, you can see that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. Now this entire green portion in the middle is all being classified as green. And then, once we move to the K=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. The white regions are where there was no majority among the k-nearest neighbors. 

![IMG-0007](https://user-images.githubusercontent.com/33539468/86518098-727f2480-be69-11ea-973c-4b971f0089c5.jpg)

So far we've talked about L1 distance which takes the sum of absolute values between the pixels. But another common choice is the L2 or Euclidean distance where you take the square root of the sum of the squares. Different distance metrics make different assumptions about the underlying geometry or topology that you'd expect in the space. One interesting thing to point out between these two metrics in particular, is that the L1 distance depends on your choice of a coordinate system. If you were to rotate the coordinate frame that would actually change the L1 distance between the points. Whereas changing the coordinates frame in the L2 distance doesn't matter. Its' the same thing no matter what your coordinate frame is. If input features or individual entries in a vector have some important meaning for a task, then somehow L1 might be a more natural fit. But if it's just a generic vector in some space and you don't know which of the different elements, you don't know what they actually mean, then maybe L2 is slightly more natural. 

## Setting Hyperparameters

Then the question is, once you're actually trying to use this algorithm in practice, there's several choices you need to make. How do you make different choices of K or distance metrics for your problem and for your data? These choices of things like K and the distance metric, we call hyperparameters, because they are not necessarily learned from the training. Instead these are choices about your algorithm that you make ahead of time and there's no way to learn them directly from the data. How do you set these things in practice? The simple thing that most people do is to try different values of hyperparameters for your data and figure out which one works best. 

![IMG-0008](https://user-images.githubusercontent.com/33539468/86518099-727f2480-be69-11ea-863e-66d236e3c8fe.jpg)

The point of machine learning systems is that we want to know how our algorithm will perform. The point of test set is to give us some estimate of how our method will do on unseen data that's coming out from the wild. And if we use this strategy of training many different algorithms with different hyperparameters, and selecting the one which does the best on the test data, then, it's possible that we may have just picked the right set of hyperparameters that caused our algorithm to work quite well on this testing set. But now our performance on this test set will no longer be representative of performance of now, unseen data. So dividing a total data to 2 groups, which are training and test dataset, is a bad idea. What is much more common is to actually split your data into 3 different sets. You'll partition most of your data into a training set and then you'll create a validation set and a test set. Now we typically do is train an algorithm with many different choices of hyperparameters on the training set, evaluate on the validation set, and now pick the set of hyperparameters which performs best on the validation set. Finally, take that best performing classifier on the validation set and run it once on the test set. 

## Cross validation

![IMG-0009](https://user-images.githubusercontent.com/33539468/86518100-7317bb00-be69-11ea-9bfd-3b3acd6bd951.jpg)

Another strategy for setting hyperparameters is called cross validation. This is commonly used in small datasets, not used so much in deep learning. Here the idea is we're going to hold out some test data to use at the very end, and for the rest of the data, rather than splitting it into a single training and validation partition, instead, we can split our training data into many different folds. For example, we use 5-fold cross validation, so you would train an algorithm with one set of hyperparameters on the first 4 folds, evaluate the performance on fold 5, and then retrain the algorithm on folds 1, 2, 3, and 5, evaluate on fold 4 and cycle through all the different folds. When you do it in this way, you get much higher confidence about which hyperparameters are going to perform more robustly. However in usual deep learning process, training is computationally expensive in this way.

![IMG-0010](https://user-images.githubusercontent.com/33539468/86518101-7317bb00-be69-11ea-9bdf-2b4d4657c5d4.jpg)

Here, on the X axis, we are showing the value of K for a k-nearest neighbor classifier on some problem, and now on the Y axis, we are showing the accuracy of classifier on some dataset for different values of K. In this case, we've done 5 fold cross validation over the data, so for the each value of K we have 5 different examples of how well this algorithm is doing. Using k-fold algorithm is a one way to help quantify the accuracy on a test data. We can see the variance of how this algorithm performs on different of the validation folds. And that gives you some sense of not just what is the best, but also what is the distribution of that performance. Here, K=7 is probably the best performance for this problem. 

## Problems of K-nearest neighbors algorithm: 
### slow at test time, lack of performance of distance metrics, and the curse of dimensionality

K-nearest neighbors classifier on images are actually almost never used in practice. One problem is that it's very slow at test time, which is kind of reverse of what we want, which we talked about earlier. Another problem is that distance metrics such as L1 and L2 distances are really not a very good way to measure distances between images. These vectorial distance functions do not correspond very well to perceptual similarity between images. Another problem with the k-nearest neighbor classifier has to do with the curse of dimensionality. We need training examples to cover the space quite densely. Otherwise the nearest neighbors could actually be quite far away and might not be very similar to testing points. The problem is, that densely covering the space means that we need a number of training examples which is exponential in the dimension of the problem. For example, if we're in 1 dimension, then you maybe only need 4 training samples, to densely cover the space. But if we move to 2 dimensions, then, we need 4*4 examples to densely cover the space. And if we move to 3, 4, 5, many more dimensions, the number of samples that we need to densely cover the space grows exponentially with the dimension. 

## Linear classification

![IMG-0012](https://user-images.githubusercontent.com/33539468/86518102-73b05180-be69-11ea-845a-9d9f70f51acb.jpg)

Linear classification is quite a simple learning algorithm, but this will become super important and help us build up to whole neural networks. You can have different kind of components of neural networks and you can stick these components together to build these large different towers of convolutional networks. One of the most basic building blocks in various types of deep learning applications is this linear classifier. So now, our parametric model has 2 different components. It's going to take in this image, maybe, of a cat on the left, and we usually write it as X for input data. The other input is a set of parameters, or weights, which is usually called W, also sometimes theta, depending on the literature. The function which takes in both the data X and the parameters W, will spit out now 10 numbers of describing what are the scores corresponding to each of those 10 categories of CIFAR-10. In contrast with KNN which requires whole training dataset for comparison to test data, we only need this W at test time in parametric approach . So this allows our models to be more efficient and actually run on maybe small devices like phones. Sometimes we'll often add a bias term which will be a constant vector of 10 elements that does not interact of the training data and instead just gives us some sort of data independent preferences for some classes over another. If your dataset was unbalanced and had more cats than dogs, for example, then the bias elements corresponding to cat would be higher than the other ones. 


Here's an example of how linear classifier works in one image.

![IMG-0013](https://user-images.githubusercontent.com/33539468/86518103-73b05180-be69-11ea-99fc-d0fc13067aba.jpg)


## Hard cases for a linear classifier

![IMG-0014](https://user-images.githubusercontent.com/33539468/86518104-7448e800-be69-11ea-96c0-f0f14ce20963.jpg)

Another viewpoint of the linear classifier is to go back to this idea of images as points in a high dimensional space. And you can imagine that each of images is something like a point in this high dimensional space. And now the linear classifier is putting in linear decision boundaries to try to draw linear separation between one category and rest of the categories. When you think about linear classification from this high dimensional point of view, you can start to see again what are some of the problems that might come up with linear classification. One example, on the left here, is that there are 2 classes and there's no way to draw a single linear operation to separate the blue from the red. So this kind of a parity problem of separating odds from evens is something that linear classification really struggles with traditionally. Other situations where a linear classifier really struggles are multimodal situations.


## Reference

[1] <https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk>